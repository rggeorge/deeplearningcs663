import pandas as pd
import sklearn.neighbors as nb
import sklearn.model_selection as ms
import matplotlib.pyplot as plt
from sklearn import tree


K = [1, 5, 10, 15]

df = pd.read_table('data_seed.dat', sep='\t', error_bad_lines=False)

N = df.shape[0]


# k-fold cross-validation

five_error = []

for k in K:
	record = []
	kf = ms.KFold(n_splits=5)
	for train_index, test_index in kf.split(df):
		tree = nb.KDTree(df.iloc[train_index])
		for i in range(0,len(test_index)):
			row = df.iloc[[test_index[i]]]
			dist, ind = tree.query(row, k=k)
			neighbor_vote = df.iloc[ind.reshape(-1,),7].mode()		
			record.append([row.iloc[0,7], neighbor_vote[0]])
	five_error.append(float(sum([j[0]==j[1] for j in record]))/N) 

# leave one out
loo_error = []

for k in K:
	record = []
	kf = ms.KFold(n_splits=N)
	for train_index, test_index in kf.split(df):
		tree = nb.KDTree(df.iloc[train_index])
		for i in range(0,len(test_index)):
			row = df.iloc[[test_index[i]]]
			dist, ind = tree.query(row, k=k)
			neighbor_vote = df.iloc[ind.reshape(-1,),7].mode()		
			record.append([row.iloc[0,7], neighbor_vote[0]])
	loo_error.append(float(sum([j[0]==j[1] for j in record]))/len(record)) 

plt.plot(K,five_error)
plt.plot(K, loo_error)
plt.show()


# Decision trees
for k in K:
	results = pd.DataFrame(columns=['predicted', 'actual'])
	kf = ms.KFold(n_splits=5)
	n = 0
	m = 0
	for train_index, test_index in kf.split(df):
		clf = tree.DecisionTreeClassifier()
		clf = clf.fit(df.iloc[train_index,0:7], df.iloc[train_index,7])
		m += len(test_index)
		results.loc[n:m,'predicted'] = clf.predict(df.iloc[test_index,0:7])
		results.loc[n:m,'actual'] = df.iloc[test_index,7]
		n = m


predicted = [p[0] for p in predicted]
actual = [a[0] for a in actual]

